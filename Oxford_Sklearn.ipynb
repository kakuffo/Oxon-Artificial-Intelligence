{"cells":[{"cell_type":"markdown","metadata":{"id":"B6kECm8TS7aY"},"source":["# Introduction\n","\n","**Python** is one of the most used programming languages more for machine learning and deep learning. These are some reasons why python is used for machine learning:\n","\n","* Python is a simple and consistent programming language\n","* It contains a variety of libraries and frameworks\n","* Can be performed practically on any platform.\n","\n","In order to benefit from the flexible possibilities offered by the Python programming language, there are a set of libraries which are mostly used:\n","\n","* Scikitlearn\n","* Numpy\n","* Pandas\n","* Matplotlib\n"]},{"cell_type":"markdown","metadata":{"id":"BzaMsRHAyjuF"},"source":["# 1. Get started with Sklearn\n","\n","**Scikit-learn** is one of the most useful libraries for machine learning in Python. The sklearn library contains a lot of efficient tools for machine learning and statistical modeling including classification, regression, clustering and dimensionality reduction.\n","\n","We will be presenting different functions and possibilities that you can use with sklearn from python.\n","\n","First step is to implement the package into the workplace."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m_TBqtQeCF_D"},"outputs":[],"source":["#import scikit-learn\n","import sklearn\n","#Importing pandas, numpy, and matplotlib libraries\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import numpy as np\n","#Importing all preexisting datasets provided by sklearn \n","from sklearn import datasets\n","#From the \"ensemble\" models of sklearn we import the random forest classifier \n","from sklearn.ensemble import RandomForestClassifier\n","import warnings\n","warnings.simplefilter(\"ignore\")"]},{"cell_type":"markdown","metadata":{"id":"TgbupQ1VjeBl"},"source":["# 2. Case study "]},{"cell_type":"markdown","metadata":{"id":"wdNu9sXFT9SC"},"source":["In order to understand and get used with  sklearn, it is better working with an application. In this notebook, we will be using the wine dataset. "]},{"cell_type":"markdown","metadata":{"id":"QMnLwcHhDAEl"},"source":["# 3. Load the dataset you want: "]},{"cell_type":"code","execution_count":7,"metadata":{"id":"v6gjAs1TC5PB"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'dataiku'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn [7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#From sklearn dataset we specify which of these datasets we are using \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdataiku\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdataiku\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pandasutils \u001b[38;5;28;01mas\u001b[39;00m pdu\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dataiku'"]}],"source":["#From sklearn dataset we specify which of these datasets we are using \n","import dataiku\n","import pandas as pd, numpy as np\n","from dataiku import pandasutils as pdu\n","\n","data= datasets.load_wine() \n","print(data.keys())"]},{"cell_type":"markdown","metadata":{"id":"Pn3QtRdtDFoC"},"source":["Note: If you want to discover the list of datasets offered by sklearn you can check the references section for more infos. \n","\n","Note: You can also upload your own dataset using pandas, as described in the links in the references section.\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"1PNT2assiZEa"},"source":["# 4. Data exploration"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4CFqj_42EpsB"},"outputs":[],"source":["#This function is a full description \"package\" it gives all infos you can need to know about your data\n","print(data.DESCR)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_L8loNO3Nvq5"},"outputs":[],"source":["#Defining the loaded dataset as a dataframe (dataframe: 2-dimensional labeled data structure with columns of potentially different types)\n","df = pd.DataFrame(data.data, columns=data.feature_names) \n","#Specifying the target is this dataset \n","df['target'] = data.target\n","#Displaying the first 5 lines of the dataset using head() function (5 is a default value)\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aK7CaTXFOE9p"},"outputs":[],"source":["#Almost like DESCR function it gives infos about the variables types and number of null values.\n","df.info() "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lmnbLHkBOHz4"},"outputs":[],"source":["#This function enables to split the dataset into a training and testing sets.\n","from sklearn.model_selection import train_test_split \n","#iloc is one of the features of dataframe by pandas, Purely integer-location based indexing for selection by position.\n","#In below code we are specifying the target variable (y) and features (X)\n","X, y = df.iloc[:, 1:].values, df.iloc[:, 0].values \n","#Splitting the dataset into training and testing sets \n","X_train, X_test, y_train, y_test = \\\n","        train_test_split(X, y, test_size=0.3, random_state=0)\n","#Displaying the length of the X_test and X_train\n","len(X_test), len(X_train)"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"7hgkOhi-QnMo"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'sklearn'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn [2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Transform features by scaling each feature to a given range.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MinMaxScaler \n\u001b[1;32m      3\u001b[0m mms \u001b[38;5;241m=\u001b[39m MinMaxScaler()\n\u001b[1;32m      4\u001b[0m  \u001b[38;5;66;03m#the fit_transform function fits to data, then transforms it\u001b[39;00m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"]}],"source":["#Transform features by scaling each feature to a given range.\n","from sklearn.preprocessing import MinMaxScaler \n","mms = MinMaxScaler()\n"," #the fit_transform function fits to data, then transforms it\n","X_train_norm = mms.fit_transform(X_train)\n","X_test_norm = mms.transform(X_test)\n","\n","X_train_norm[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8qnb-mAVQsp_"},"outputs":[],"source":["#Displaying the first raw of X_test after normalization \n","X_test_norm[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fOhYT5fdQvJX"},"outputs":[],"source":["# \"Standardize\" features by removing the mean and \"scaling\" to unit variance.\n","from sklearn.preprocessing import StandardScaler \n","stdsc = StandardScaler()\n","X_train_std = stdsc.fit_transform(X_train)\n","X_test_std = stdsc.transform(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YDMe7CFXQ1bf"},"outputs":[],"source":["X_train_std[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5yeqVqMAQ3cn"},"outputs":[],"source":["xx = np.arange(len(X_train_std))\n","yy1 = X_train_norm[:,1]\n","yy2 = X_train_std[:,1]\n","#PLot normalised data and standardized data.\n","plt.scatter(xx, yy1, color='g') \n","plt.scatter(xx, yy2, color='y')"]},{"cell_type":"markdown","metadata":{"id":"fqBxjrhP58hw"},"source":["# 5. Training the model "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u1X2IjuARSJa"},"outputs":[],"source":["#importing prebuild logistic regression model \n","from sklearn.linear_model import LogisticRegression \n","#importing all preprocessing functions offered by sklearn\n","from sklearn import preprocessing \n","#contains many utilities : https://scikit-learn.org/stable/modules/classes.html#module-sklearn.utils \n","from sklearn import utils "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-J95S_nPRi34"},"outputs":[],"source":["#Defining the penalty or loss function as Ridge Regression (l2)\n","LogisticRegression(penalty='l2')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gzsrdm90Rnia"},"outputs":[],"source":["#Encoding the labels (encoding is techniques to transform categorical values to numerical)\n","lab_enc = preprocessing.LabelEncoder()\n","encoded1 = lab_enc.fit_transform(y_train)\n","encoded2=lab_enc.fit_transform(y_test)\n","#Specifying which colomns contain the labels of the features\n","feat_labels = df.columns[1:]\n","#Defining the classifier\n","forest = RandomForestClassifier(n_estimators=10000, random_state=0, n_jobs=-1)\n","forest.fit(X_train, encoded1)\n","#Defining/calculating and plotting the importance of features (reflects at which point does a feature impact the output of the classifier)\n","importances = forest.feature_importances_\n","indices = np.argsort(importances)[::-1]\n","\n","for f in range(X_train.shape[1]):\n","    print(\"%2d) %-*s %f\" % (f + 1, 30, feat_labels[f],importances[indices[f]]))\n","\n","plt.title('Feature Importances')\n","plt.bar(range(X_train.shape[1]), importances[indices], \n","                     color='cyan', align='center')\n","plt.xticks(range(X_train.shape[1]),feat_labels, rotation=90)\n","plt.xlim([-1, X_train.shape[1]])\n","plt.tight_layout()\n","plt.show()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fy8Y8U2efs3d"},"outputs":[],"source":["logi_regr = LogisticRegression(penalty='l2', C=0.1)\n","logi_regr.fit(X_train_std, encoded1)\n","print('Training accuracy:', logi_regr.score(X_train_std, encoded1))\n","print('Test accuracy:', logi_regr.score(X_test_std, encoded2))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PlfePVsqOQoY"},"outputs":[],"source":["logi_regr.intercept_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TsjEvflvcwFG"},"outputs":[],"source":["logi_regr.coef_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9-d6VQQYcyNU"},"outputs":[],"source":["#Plotting the evolution of the classifier's weights : \n","fig = plt.figure()\n","ax = plt.subplot(111)\n","colors = ['blue', 'green', 'red', 'cyan',\n","'magenta', 'yellow', 'black',\n","'pink', 'lightgreen', 'lightblue',\n","'gray', 'indigo', 'orange']\n","weights, params = [], []\n","for c in np.arange(0, 6):\n","    lr = LogisticRegression(penalty='l2',C=10**c, random_state=0)\n","    lr.fit(X_train_std, encoded1)\n","    weights.append(lr.coef_[1])\n","    params.append(10**c)\n","weights = np.array(weights)\n","for column, color in zip(range(weights.shape[1]), colors):\n","    plt.plot(params, weights[:, column],\n","        label=df.columns[column+1], color=color)\n","plt.axhline(0, color='black', linestyle='--', linewidth=3)\n","plt.xlim([10**(-5), 10**5])\n","plt.ylabel('weight coefficient')\n","plt.xlabel('C')\n","plt.xscale('log')\n","plt.legend(loc='upper left')\n","plt.legend(loc='upper center',\n","bbox_to_anchor=(1.38, 1.03),\n","ncol=1, fancybox=True)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"s9rE1B0Ciquq"},"source":["# 6. Building ML models using sklearn package"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CVZas_XuxDS6"},"outputs":[],"source":["#those modules can be uploaded seperatly or in the same time it won t change much. the datasets contain vanilla datasets like Boston House Prices, Iris dataset and others.\n","# the neighbors module contain such functions to implement the k-nearest neighbors algorithm while the preprocessing contain many prebuilt functions to process the data with already conventioned techniques.\n","from sklearn import neighbors, datasets, preprocessing  \n"," #train_test_split is a specific function in the model selection submodule of sklearn that enables you to split your data according to a norm and split rate that you choose.\n","from sklearn.model_selection import train_test_split \n","#metrics of sklearn enebales you to use prebuilt metrics for classification, regression...\n","from sklearn.metrics import accuracy_score \n","\n","#In the following are some example of how to use the previously imported librarries: \n","#You will notice that only with few lines of code we can upload the dataset: \n","\n","iris = datasets.load_iris()\n","\n","#We then decided what are the predictors and reponse : \n","\n","X, y = iris.data[:, :2], iris.target \n","\n","#We split the dataset into training and testing dataset \n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=33)\n","\n","#We processed the data applying standardization using the standardscaler function to standardize features by removing the mean and scaling to unit variance\n","scaler = preprocessing.StandardScaler().fit(X_train)\n","X_train = scaler.transform(X_train)\n","X_test = scaler.transform(X_test)\n","\n","#build the model, here in this case, we are using a KNN (K-Nearest Neighbors)\n","knn = neighbors.KNeighborsClassifier(n_neighbors=5)\n","\n","#Train the model using the fit function \n","knn.fit(X_train, y_train)\n","\n","#Test the model using our dataset (test set)\n","y_pred = knn.predict(X_test)\n","\n","#Evalute the model using functions from the submodule: metrics from sklearn, in this case we are using the accuracy score\n","accuracy_score(y_test, y_pred)"]},{"cell_type":"markdown","metadata":{"id":"aQudGjMoi4tQ"},"source":["If we trying to go more into details in this process: "]},{"cell_type":"markdown","metadata":{"id":"POYCGhvSU3ld"},"source":["# 7. Preprocessing data"]},{"cell_type":"markdown","metadata":{"id":"phHoJR1ChYDO"},"source":["### Standardization "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ae7G_KF5f7CD"},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler\n","scaler = StandardScaler().fit(X_train)\n","standardized_X = scaler.transform(X_train)\n","standardized_X_test = scaler.transform(X_test)"]},{"cell_type":"markdown","metadata":{"id":"J7FPqUfxhfmF"},"source":["### Normalization "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s9Hb9ALjf73D"},"outputs":[],"source":["from sklearn.preprocessing import Normalizer\n","scaler = Normalizer().fit(X_train)\n","normalized_X = scaler.transform(X_train)\n","normalized_X_test = scaler.transform(X_test)"]},{"cell_type":"markdown","metadata":{"id":"ZAq_APJdhict"},"source":["### Binarization "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-Img7nmygDqK"},"outputs":[],"source":["from sklearn.preprocessing import Binarizer\n","binarizer = Binarizer(threshold=0.0).fit(X)\n","binary_X = binarizer.transform(X)"]},{"cell_type":"markdown","metadata":{"id":"YiqgwGwXhl3N"},"source":["### Encoding Categorical Features "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n6vwQZLDgESo"},"outputs":[],"source":["from sklearn.preprocessing import LabelEncoder\n","enc = LabelEncoder()\n","y = enc.fit_transform(y)"]},{"cell_type":"markdown","metadata":{"id":"ncXb3XeChqNj"},"source":["### Imputing missing values "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qR9Bzm5NgGow"},"outputs":[],"source":["from sklearn.impute import SimpleImputer\n","imp = SimpleImputer(missing_values=0, strategy='mean')\n","imp.fit_transform(X_train)"]},{"cell_type":"markdown","metadata":{"id":"v8qOQ5eghthi"},"source":["## Generating Polynomial features "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PVVieIxMgI9I"},"outputs":[],"source":["from sklearn.preprocessing import PolynomialFeatures\n","poly = PolynomialFeatures(5)\n","poly.fit_transform(X)"]},{"cell_type":"markdown","metadata":{"id":"DnwYFVxxhx5W"},"source":["# 8. Training and testing data "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cPqGAn55gLIS"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=0)"]},{"cell_type":"markdown","metadata":{"id":"hlDxajilh13L"},"source":["# 9. Model creation "]},{"cell_type":"markdown","metadata":{"id":"xpUpNMWUVKGe"},"source":["## Linear regression "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1I0FMq-RgOTI"},"outputs":[],"source":["from sklearn.linear_model import LinearRegression\n","lr = LinearRegression(normalize=True)"]},{"cell_type":"markdown","metadata":{"id":"_paZ26uTiDH3"},"source":["## Support Vector Machines "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HTvZ-TW9glYz"},"outputs":[],"source":["from sklearn.svm import SVC\n","svc = SVC(kernel='linear')"]},{"cell_type":"markdown","metadata":{"id":"mO_zEeHciI_t"},"source":["## Naive Bayes "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j0m1o1OrgnEH"},"outputs":[],"source":["from sklearn.naive_bayes import GaussianNB\n","gnb = GaussianNB()"]},{"cell_type":"markdown","metadata":{"id":"fZt88Vm7iNeD"},"source":["## KNN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"txuBQ69Ygoh4"},"outputs":[],"source":["from sklearn import neighbors\n","knn = neighbors.KNeighborsClassifier(n_neighbors=5)"]},{"cell_type":"markdown","metadata":{"id":"eQnM0A7RiRLr"},"source":["# 10. USING Principal Componenent Analysis "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LtIZXKXrgqBa"},"outputs":[],"source":["from sklearn.decomposition import PCA\n","pca = PCA(n_components=0.95)"]},{"cell_type":"markdown","metadata":{"id":"KRpsxtwxS05P"},"source":["# 11. References "]},{"cell_type":"markdown","metadata":{"id":"0io3HT1bSyba"},"source":["[1] https://scikit-learn.org/stable/install.html\n","\n","\n","\n","[2] https://scikit-learn.org/stable/\n","\n","\n","\n","[3] https://scikit-learn.org/stable/datasets/toy_dataset.html\n","\n","\n","\n","[4] https://realpython.com/tutorials/databases/ \n","\n","\n","\n","[5] https://github.com/asavinov/machine-learning-and-data-processing\n","\n","\n","\n","[6] http://totoharyanto.staff.ipb.ac.id/files/2012/10/Building-Machine-Learning-Systems-with-Python-Richert-Coelho.pdf\n","\n","\n","\n","[7] https://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html#sphx-glr-auto-examples-classification-plot-digits-classification-py\n","\n","\n","\n","\n","[8] https://scikit-learn.org/stable/modules/neighbors.html\n","\n","\n","\n","[9] https://scikit-learn.org/stable/modules/grid_search.html\n","\n","\n","\n","\n","[10] https://scikit-learn.org/stable/modules/model_evaluation.html\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPOb08ETfWjSzkVXLcm86we","collapsed_sections":["BzaMsRHAyjuF","TgbupQ1VjeBl","QMnLwcHhDAEl","1PNT2assiZEa","fqBxjrhP58hw","s9rE1B0Ciquq","POYCGhvSU3ld","DnwYFVxxhx5W","hlDxajilh13L","eQnM0A7RiRLr","KRpsxtwxS05P"],"provenance":[]},"kernelspec":{"display_name":"Python 3.10.6 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}}},"nbformat":4,"nbformat_minor":0}
